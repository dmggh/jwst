% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}

\usepackage{enumitem}
\setlistdepth{15}

\usepackage{amsmath}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}

% In the parameters section, place a newline after the Parameters
% header
\usepackage{expdlist}
\let\latexdescription=\description
\def\description{\latexdescription{}{} \breaklabel}

% Make Examples/etc section headers smaller and more compact
\makeatletter
\titleformat{\paragraph}{\normalsize\py@HeaderFamily}%
            {\py@TitleColor}{0em}{\py@TitleColor}{\py@NormalColor}
\titlespacing*{\paragraph}{0pt}{1ex}{0pt}

% Fix footer/header
\@ifundefined{chaptermark}{%
\newcommand{\chaptermark}[1]{\markboth{\MakeUppercase{\thechapter.\ #1}}{}}
}{%
\renewcommand{\chaptermark}[1]{\markboth{\MakeUppercase{\thechapter.\ #1}}{}}
}

\@ifundefined{chaptermark}{%
\newcommand{\sectionmark}[1]{\markright{\MakeUppercase{\thesection.\ #1}}}
}{%
\renewcommand{\sectionmark}[1]{\markright{\MakeUppercase{\thesection.\ #1}}}
}

\@ifundefined{TSR}{}{%
\renewcommand{\py@HeaderFamily}{\rmfamily\bfseries}
\definecolor{TitleColor}{rgb}{0,0,0}
\renewcommand{\appendix}{\par
  \setcounter{section}{0}%
  \inapptrue%
  \renewcommand\thesection{\@Alph\c@section}}
}

\makeatother

% Make the pages always arabic, and don't do any pages without page
% numbers
\pagenumbering{arabic}
\pagestyle{plain}


\title{CRDS Server Documentation}
\date{July 18, 2014}
\release{1.1}
\author{STScI}
\newcommand{\sphinxlogo}{\includegraphics{stsci_logo.pdf}\par}
\renewcommand{\releasename}{Release}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\def\PYG@tok@gd{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\def\PYG@tok@gu{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\def\PYG@tok@gt{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.25,0.82}{##1}}}
\def\PYG@tok@gs{\let\PYG@bf=\textbf}
\def\PYG@tok@gr{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\def\PYG@tok@cm{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\def\PYG@tok@vg{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@m{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@mh{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@cs{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\colorbox[rgb]{1.00,0.94,0.94}{##1}}}
\def\PYG@tok@ge{\let\PYG@it=\textit}
\def\PYG@tok@vc{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@il{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@go{\def\PYG@tc##1{\textcolor[rgb]{0.19,0.19,0.19}{##1}}}
\def\PYG@tok@cp{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@gi{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\def\PYG@tok@gh{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\def\PYG@tok@ni{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\def\PYG@tok@nl{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\def\PYG@tok@nn{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\def\PYG@tok@no{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\def\PYG@tok@na{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@nb{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@nc{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\def\PYG@tok@nd{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\def\PYG@tok@ne{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@nf{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\def\PYG@tok@si{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\def\PYG@tok@s2{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@vi{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@nt{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\def\PYG@tok@nv{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@s1{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@gp{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\def\PYG@tok@sh{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@ow{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@sx{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\def\PYG@tok@bp{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@c1{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\def\PYG@tok@kc{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@c{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\def\PYG@tok@mf{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@err{\def\PYG@bc##1{\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{##1}}}
\def\PYG@tok@kd{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@ss{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\def\PYG@tok@sr{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\def\PYG@tok@mo{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@mi{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@kn{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@o{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\def\PYG@tok@kr{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@s{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@kp{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@w{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\def\PYG@tok@kt{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\def\PYG@tok@sc{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@sb{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@k{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@se{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@sd{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}



\chapter{Introduction}
\label{server_guide:introduction}\label{server_guide::doc}\label{server_guide:crds-server-guide}
This guide is intended to introduce the CRDS servers for the purposes of maintenance and emergency backup for Todd.
Additional documentation about using CRDS and the servers can be located on the servers under the question mark icon
at the top right each page.

Historical Institute contacts for the CRDS servers include:
\begin{itemize}
\item {} 
Todd Miller:         primary CRDS and CRDS server application developer.

\item {} 
Pey-Lian Lim:        intial JWST cloned references and rules, jwst\_gentools branch

\item {} 
Jonathan Eisenhamer  table differencing and reprocessing implications

\item {} 
Patrick Taylor:      web proxies, ssl support, and server rc/reboot script coordination

\item {} 
Thomas Walker:       initial VM creation, file systems, and Isilon storage setup.

\end{itemize}


\chapter{Servers}
\label{server_guide:servers}

\section{CRDS pseudo-user and group}
\label{server_guide:crds-pseudo-user-and-group}
The CRDS servers run as the no-login user ``crds''.  The CRDS  crds\_server script contains the following:

\begin{Verbatim}[commandchars=\\\{\}]
\% ssh -A -t \$1.stsci.edu /usr/bin/sudo /bin/su - crds
\end{Verbatim}

and is invoked like this:

\begin{Verbatim}[commandchars=\\\{\}]
\% crds\_server plhstcrdsv1
\# crds\_server \textless{}VM-hostname\textgreater{}
\end{Verbatim}

The ssh first logs into your normal user account on the server VM, then su's you to CRDS.   It is not possible
to directly login to the crds user.

Server maintainers need to get membership in group crdsoper and ``sudo su crds'' access on
the appropriate server VMs.

Members of DSB share the crdsoper group and use it to modify the CRDS server file delivery
directory described below or copy files directly to their ingest directories.


\section{Virtual Machines and URLs}
\label{server_guide:virtual-machines-and-urls}
The CRDS servers exist on virtual machines,  run Apache servers and Django via mod\_wsgi,
are backed by memcached as a memory caching optimization for frequent traffic.  Currently
there are 6 VMs and servers:  (hst, jwst) x (dev, test, ops):

\begin{tabulary}{\linewidth}{|L|L|L|L|L|}
\hline
\textbf{
observatory
} & \textbf{
use
} & \textbf{
host/vm
} & \textbf{
direct port
} & \textbf{
url
}\\\hline

hst
 & 
django
 & 
localhost
 & 
8000
 & 
\href{http://localhost:8000}{http://localhost:8000}
\\\hline

hst
 & 
dev
 & 
dlhstcrdsv1
 & 
8001
 & 
\href{https://hst-crds-dev.stsci.edu}{https://hst-crds-dev.stsci.edu}
\\\hline

hst
 & 
test
 & 
tlhstcrdsv1
 & 
8001
 & 
\href{https://hst-crds-test.stsci.edu}{https://hst-crds-test.stsci.edu}
\\\hline

hst
 & 
ops
 & 
plhstcrdsv1
 & 
8001
 & 
\href{https://hst-crds.stsci.edu}{https://hst-crds.stsci.edu}
\\\hline

jwst
 & 
django
 & 
localhost
 & 
8000
 & 
\href{http://localhost:8000}{http://localhost:8000}
\\\hline

jwst
 & 
dev
 & 
dljwstcrdsv1
 & 
8001
 & 
\href{https://jwst-crds-dev.stsci.edu}{https://jwst-crds-dev.stsci.edu}
\\\hline

jwst
 & 
test
 & 
tljwstcrdsv1
 & 
8001
 & 
\href{https://jwst-crds-test.stsci.edu}{https://jwst-crds-test.stsci.edu}
\\\hline

jwst
 & 
ops
 & 
pljwstcrdsv1
 & 
8001
 & 
\href{https://jwst-crds.stsci.edu}{https://jwst-crds.stsci.edu}
\\\hline
\end{tabulary}


For debug purposes the servers can be accessed by bypassing the proxy using VM-based URLs such
as \href{https://plhstcrdsv1.stsci.edu:8001/}{https://plhstcrdsv1.stsci.edu:8001/}.  These direct URLs are visible on site only.  Only the OPS
server URLs will be visible off site.

See CRDS\_server/sources/site\_config.py to verify server configuration.


\section{CRDS Client Configuration Scripts}
\label{server_guide:crds-client-configuration-scripts}
Configuring the client to work with various CRDS servers can be accomplished using scripts define in the CRDS client
under trunk/envs:

\begin{tabulary}{\linewidth}{|L|L|L|L|}
\hline
\textbf{
Script
} & \textbf{
CRDS\_SERVER\_URL
} & \textbf{
CRDS\_PATH
} & \textbf{
Purpose
}\\\hline

default
 & 
\href{https://crds-serverless-mode.stsci.edu}{https://crds-serverless-mode.stsci.edu}
 & 
readonly /grp/crds/cache
 & 
Default no env, serverless jwst ops
\\\hline

env-forwarded.csh
 & 
\href{https://localhost:8001}{https://localhost:8001}
 & 
\$HOME/crds\_cache\_forwarded
 & 
ssh port forwarded private server
\\\hline

env-local.csh
 & 
\href{https://localhost:8000}{https://localhost:8000}
 & 
\$HOME/crds\_cache\_local
 & 
Django development server
\\\hline

hst-crds-dev.csh
 & 
\href{https://hst-crds-dev.stsci.edu}{https://hst-crds-dev.stsci.edu}
 & 
\$HOME/crds\_cache\_dev
 & 
Connected hst dev local cache
\\\hline

hst-crds-test.csh
 & 
\href{https://hst-crds-test.stsci.edu}{https://hst-crds-test.stsci.edu}
 & 
\$HOME/crds\_cache\_test
 & 
Connected hst test local cache
\\\hline

hst-crds-ops.csh
 & 
\href{https://hst-crds.stsci.edu}{https://hst-crds.stsci.edu}
 & 
\$HOME/crds\_cache\_ops
 & 
Connected hst ops local cache
\\\hline

jwst-crds-dev.csh
 & 
\href{https://jwst-crds-dev.stsci.edu}{https://jwst-crds-dev.stsci.edu}
 & 
\$HOME/crds\_cache\_dev
 & 
Connected jwst dev local cache
\\\hline

jwst-crds-test.csh
 & 
\href{https://jwst-crds-test.stsci.edu}{https://jwst-crds-test.stsci.edu}
 & 
\$HOME/crds\_cache\_test
 & 
Connected jwst test local cache
\\\hline

jwst-crds-ops.csh
 & 
\href{https://jwst-crds.stsci.edu}{https://jwst-crds.stsci.edu}
 & 
\$HOME/crds\_cache\_ops
 & 
Connected jwst ops local cache
\\\hline

crds-readonly.csh
 & 
\href{https://crds-serverless-mode.stsci.edu}{https://crds-serverless-mode.stsci.edu}
 & 
readonly /grp/crds/cache
 & 
Disconnected, complete ops cache
\\\hline

hst-crds-readonly.csh
 & 
\href{https://hst-crds.stsci.edu}{https://hst-crds.stsci.edu}
 & 
readonly /grp/crds/hst
 & 
Connected, complete ops cache
\\\hline

jwst-crds-readonly.csh
 & 
\href{https://jwst-crds.stsci.edu}{https://jwst-crds.stsci.edu}
 & 
readonly /grp/crds/jwst
 & 
Connected, complete ops cache
\\\hline
\end{tabulary}


At present only connected clients can resolve symbolic/date-based contexts other than -operational.

Even without env settings,  many tools can guess the appropriate cache and ops server url based on files.


\chapter{Mailing Lists}
\label{server_guide:mailing-lists}
The following CRDS mailing lists are defined on behalf of CRDS:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{
mailing list
} & \textbf{
moderator
} & \textbf{
purpose
}\\\hline

\href{mailto:crds@stsci.edu}{crds@stsci.edu}
 & 
mcmaster
 & 
comm between INS and CRDS team, also accidental use
\\\hline

\href{mailto:crds\_team@stsci.edu}{crds\_team@stsci.edu}
 & 
mcmaster
 & 
comm within CRDS delivery team, DSB, archive, pipelines
\\\hline

\href{mailto:crds\_datamng@stsci.edu}{crds\_datamng@stsci.edu}
 & 
mcmaster
 & 
common destination for affected datasets output
\\\hline

\href{mailto:crds-servers@stsci.edu}{crds-servers@stsci.edu}
 & 
jmiller
 & 
little used,  new source address for affected datasets,  server details + news
\\\hline

\href{mailto:crds\_hst\_ops\_reprocessing@stsci.edu}{crds\_hst\_ops\_reprocessing@stsci.edu}
 & 
jmiller
 & 
affected datasets output list, hst ops
\\\hline

\href{mailto:crds\_hst\_test\_reprocessing@stsci.edu}{crds\_hst\_test\_reprocessing@stsci.edu}
 & 
jmiller
 & 
affected datasets output list, hst test
\\\hline

\href{mailto:crds\_jwst\_ops\_reprocessing@stsci.edu}{crds\_jwst\_ops\_reprocessing@stsci.edu}
 & 
jmiller
 & 
affected datasets output list, jwst ops
\\\hline

\href{mailto:crds\_jwst\_test\_reprocessing@stsci.edu}{crds\_jwst\_test\_reprocessing@stsci.edu}
 & 
jmiller
 & 
affected datasets output list, jwst test
\\\hline
\end{tabulary}


These are all closed lists.  Most critical are crds\_team and crds\_datamng.   The reprocessing lists are intended and/or used to drive
automated systems so be careful with traffic on those.


\chapter{Server File Systems}
\label{server_guide:server-file-systems}

\section{Cross-Server Shared Home (/home/crds)}
\label{server_guide:cross-server-shared-home-home-crds}
The VMs and servers share a common /home/crds directory which has potential as a single point failure.  In particular,
critical shell rc scripts (.setenv) are shared by all servers and must be updated with extreme care because
any error instantly affects all 6 servers.

/home/crds is useful for communicating information between VMs during setup and maintenance.

The RC scripts are version controlled with the server source code in the directory ``hosts'' under the names dot\_setenv
and rc\_script.


\subsection{.setenv}
\label{server_guide:setenv}
The CRDS user runs under /bin/tcsh and executes .setenv for CRDS-server specific initializations.   Note that
\$HOME/.setenv is shared across all CRDS servers and should be modified with extreme caution.   The environment
variables defined to differentiate the 6 CRDS servers are,  for example for JWST DEV:

\begin{Verbatim}[commandchars=\\\{\}]
CRDS\_PROJECT        jwst
CRDS\_USECASE        dev
CRDS\_SERVER         dljwstcrdsv1
CRDS                /crds/data1/dljwstcrdsv1
PATH                /crds/data1/dljwstcrdsv1/CRDS\_server/host   /crds/data1/dljwstcrdsv1/crds\_stacks/crds\_11/bin
CRDS\_STACK          /crds/data1/dljwstcrdsv1/crds\_stacks/crds\_11
CRDS\_AFFECTED...    jmiller@stsci.edu eisenhamer@stsci.edu
CRDS\_IFS            /ifs/crds/jwst/dev
CRDS\_FILE\_CACHE     /ifs/crds/jwst/dev/file\_cache
CRDS\_SERVER\_FILES   /ifs/crds/jwst/dev/server\_files
META\_PREFIX         /crds/data1/dljwstcrdsv1/crds\_stacks/crds\_11
\end{Verbatim}

Additional environment variables, particularly those related to server installation, are defined in
\$\{CRDS\}/CRDS\_server/env.csh.

META\_PREFIX is roughly equivalent to /usr/local,  the common value passed to --prefix in ./configure,  etc.,
for building the server Python stack.


\subsection{.alias}
\label{server_guide:alias}
CRDS augments the standard .alias file with these aliases for moving around the file system:

\begin{Verbatim}[commandchars=\\\{\}]
\#  Source code areas
alias crds       "cd \$\PYGZob{}CRDS\PYGZcb{}/CRDS"
alias server     "cd \$\PYGZob{}CRDS\PYGZcb{}/CRDS\_server"
alias stack      "cd \$\PYGZob{}CRDS\_STACK\PYGZcb{}"
alias installer "cd \$\PYGZob{}CRDS\PYGZcb{}/crds\_stacks/installer3/build"

\# Server maintenance areas
alias logs       "cd \$\PYGZob{}CRDS\PYGZcb{}/server/logs"
alias backups    "cd \$\PYGZob{}CRDS\PYGZcb{}/server/db\_backups"

\# CRDS code area
alias libpython  "cd \$\PYGZob{}CRDS\PYGZcb{}/python/lib/python"

\# Server working data files areas
alias deliveries "cd \$\PYGZob{}CRDS\_SERVER\_FILES\PYGZcb{}/deliveries"
alias catalogs   "cd \$\PYGZob{}CRDS\_SERVER\_FILES\PYGZcb{}/catalogs"
alias ingest     "cd \$\PYGZob{}CRDS\_SERVER\_FILES\PYGZcb{}/ingest"
alias file\_cache "cd \$\PYGZob{}CRDS\_FILE\_CACHE\PYGZcb{}"

\# Isilon and VM file systems
alias ifs        "cd \$\PYGZob{}CRDS\_IFS\PYGZcb{}"
alias data1      "cd \$\PYGZob{}CRDS\PYGZcb{}"
\end{Verbatim}


\subsection{rc\_script}
\label{server_guide:rc-script}
The /home/crds/rc\_script is executed to restart the servers,  or shut them down,  whenever the server is rebooted.


\section{Server Static File Storage}
\label{server_guide:server-static-file-storage}
The CRDS server code and support files (Python stack, logs, monitor\_reprocessing dir) are stored on
a private VM-unique volume named after the host,  e.g.  /crds/data1.  This serves as the
./configure --prefix directory for a small number of packages not contained in the crds\_stack subdirectory.
Files within this directory tree are logically executable or in some way secret,  sensitive with respect
to server security.   Most files/subdirs are located in a subdirectory named after the host,
e.g. /crds/data1/plhstcrdsv1.


\subsection{server runtime directory}
\label{server_guide:server-runtime-directory}
A number of subdirectories are used to store files related to running Apache, logging, or backups under
e.g. /crds/data1/dlhstcrdsv1/server.


\subsubsection{conf subdirectory}
\label{server_guide:conf-subdirectory}
Apache config files are installed here at e.g. /crds/data1/dlhstcrdsv1/server/conf.  Files ssl.conf and httpd.conf.


\subsubsection{logs subdirectory}
\label{server_guide:logs-subdirectory}
There are a number of Apache logs kept at e.g. /crds/data1/dlhstcrdsv1/server/logs.  These logs record requests to
Apache and stderr output from Django views not visible to end users.


\subsubsection{db\_backups subdirectory}
\label{server_guide:db-backups-subdirectory}
The output of the CRDS\_server/tools/backup\_server script is kept here in dated subdirectories,
e.g. /crds/data1/dlhstcrdsv1/server/db\_backups/2014-05-30-033327.  These contain a backup of the CRDS server database,
catalog files, deliveries files, all mappings, the server CRDS cach config directory, and an VM rpm listing.
For use with restore\_server,  these files would need to be copied to differently named locations in \$HOME/backups
which only record the results of the last backup.


\subsubsection{wsgi-scripts subdirectory}
\label{server_guide:wsgi-scripts-subdirectory}
The mod\_wsgi script which bridges from Apache to Django,  crds.wsgi,  is kept here,
e.g. /crds/data1/dlhstcrdsv1/server/wsgi-scripts.  Potentially other django or non-django WSGI scripts
would go here as well.


\subsubsection{run subdirectory}
\label{server_guide:run-subdirectory}
The running Apache process id is stored here.   The id of memcached should be stored here as well but isn't stored.


\subsection{database directory}
\label{server_guide:database-directory}
Files required to support operations with databases are stored in a top level static file system
subdirectory,  e.g. /crds/data1/database.   These files are secret,  effectively mode 700, and maintained
manually as part of database setup.  They're referred to by site-specific database configurations.


\subsection{CRDS client source directory}
\label{server_guide:crds-client-source-directory}
The checkout of the CRDS core library source code installed with the CRDS server is located in the static file tree
under the subdirectory CRDS and visited using the alias ``crds''.  e.g.  /crds/data1/plhstcrdsv1/CRDS.  Typically
the server uses the core library and utilities directly,  but the server is also responsible for testing the client
JSONRPC services.


\subsection{CRDS\_server source directory}
\label{server_guide:crds-server-source-directory}
The checkout of the CRDS server source code is located in the static file tree under the subdirectory CRDS\_server
and visited using the alias ``server''.  e.g. /crds/data1/plhstcrdsv1/CRDS\_server


\subsubsection{sources directory}
\label{server_guide:sources-directory}
This directory contains the Django server and application source code.

e.g. /crds/data1/plhstcrdsv1/CRDS\_server/sources
\begin{itemize}
\item {} \begin{description}
\item[{\emph{sources/configs}}] \leavevmode
contains site specific django configuration and database configuration files.  The appropriate files
are copied to sources/site\_config.py and sources/crds\_database.py at install time.   Those are then
imported into more generic configuration files sources/config.py and sources/settings.py.   The site
specific files are intended to contain the minimal information required to differentiate servers.

\end{description}

\item {} \begin{description}
\item[{\emph{sources/urls.py}}] \leavevmode
defines most of the site URLs for all applications.

\end{description}

\item {} \begin{description}
\item[{\emph{sources/settings.py}}] \leavevmode
fairly standard Django settings.py

\end{description}

\item {} \begin{description}
\item[{\emph{sources/templates}}] \leavevmode
contains web template base classes.  many applications also contain a \emph{templates} subdirectory.

\end{description}

\item {} \begin{description}
\item[{\emph{sources/static}}] \leavevmode
contains most CRDS static files,  particularly Javascript and CSS.

\end{description}

\item {} \begin{description}
\item[{\emph{sources/interactive}}] \leavevmode
is the primary web application for CRDS browsing and file submission.

\end{description}

\item {} \begin{description}
\item[{\emph{sources/jsonapi}}] \leavevmode
is the JSONRPC application which supports web services in the crds.client api.

\end{description}

\item {} \begin{description}
\item[{\emph{sources/jpoll}}] \leavevmode
application supports the Javascript logging + done polling system used for long running views,
particularly file submissions which can exceed proxy timeouts and run too long to leave a human
without info.

\end{description}

\item {} \begin{description}
\item[{\emph{sources/locking}}] \leavevmode
application for database based locks used by CRDS web logins for exclusive access to an instrument.

\end{description}

\item {} \begin{description}
\item[{\emph{sources/fileupload}}] \leavevmode
application supports the fancy file submission file upload dialogs for file submissions.

\end{description}

\item {} \begin{description}
\item[{\emph{sources/stats}}] \leavevmode
application mostly defunct django-level request logging to database,  superceded by Apache
logging.  Some parameter capture not present in current Apache configuration.

\end{description}

\end{itemize}


\subsubsection{host directory}
\label{server_guide:host-directory}
The CRDS\_server/host subdirectory is on the PATH.  It contains scripts related to cron jobs,  affected datasets
reprocessing, stack building,  server utilities, etc.   e.g. /crds/data1/plhstcrdsv1/CRDS\_server/host


\subsubsection{tools directory}
\label{server_guide:tools-directory}
The CRDS\_server/tools directory contains more complicated scripts related to server backup, restore, mirroring,
consistency checking, server initialization, user and group maintenance, etc.   The tools directory is not on the
PATH and contains more eclectic scripts developed in an unplanned manner,  basically capturing whatever I needed
to do repeatedly or had to Google.   e.g. /crds/data1/plhstcrdsv1/CRDS\_server/tools


\subsubsection{servers directory}
\label{server_guide:servers-directory}
e.g. /crds/data1/plhstcrdsv1/CRDS\_server/servers

This directory contains the Apache and mod\_wsgi configuration files which are copied by ./install to their CRDS
server installation directories.


\subsection{crds\_stacks directory}
\label{server_guide:crds-stacks-directory}
e.g. /crds/data1/plhstcrdsv1/crds\_stacks

The crds\_stacks subdirectory contains mostly stock python stack binaries and source code,  supporting third party packages
for the server application.  The CRDS server Python stack is built from source contained in the installer3 subdirectory.
Binaries are output to parallel subdirectories,  e.g. crds\_11.

An automatic nightly build and reinstall of the stack occurs on the dev and test servers so it's possible to upgrade all
the non-ops servers by updating the central installer3 repo at /eng/ssb/crds/installer3.

Independent checkouts of the repo are contained in the stacks file store for each VM. The purpose of individual VMs is
to facilitate independent configuration and test of Linux, the Python stack, and the CRDS server on each distinct VM.
The OPS servers are configured for manual updates.


\subsection{monitor\_reprocessing directory}
\label{server_guide:monitor-reprocessing-directory}
Output from the monitor\_reprocessing cron job is stored in dated subdirectories here.  Also the file old\_context.txt
which records the last known operational context against which changes are measured.  Changed old\_context.txt will
trigger an affected datasets calculation as will changing the operational context on the web site.


\section{Server Dynamic File Storage}
\label{server_guide:server-dynamic-file-storage}
For operating,  the CRDS servers require a certain amount of dynamic storage use for purposes like:
\begin{itemize}
\item {} 
holding pending archive deliveries  (deliveries, catalogs)

\item {} 
uploading files (uploads, ingest, ingest\_ssb)

\end{itemize}

The server dynamic file storage is located on the Isilon file server at:
\begin{quote}

/ifs/crds/\textless{}obsevatory\textgreater{}/\textless{}use\textgreater{}/server\_files,    e.g. /ifs/crds/hst/ops/server\_files.
\end{quote}

Since this area is actively written as a consequence of users accessing the web site,  it is kept distinct from the
code and files required to run the server.


\subsection{catalogs subdirectory}
\label{server_guide:catalogs-subdirectory}
Files submitted to the archive generate .cat file lists which are stored permanently in the catalogs directory.
Any file in CRDS is also stored in the server file cache,  so given the .cat file list the delivery can be recreated
by regenerating file links in the deliveries directory.  The catalogs directory is an internal CRDS server data store
which records file lists from past deliveries.


\subsection{deliveries subdirectory}
\label{server_guide:deliveries-subdirectory}
The deliveries directory is cross-mounted between the CRDS server VM and CRDS-archive-pipeline machines,  not
necessarily under the same path name.

Files submitted to the archive are placed in the CRDS delivery directory along with a numbered catalog file which
lists the submitted files one per line.   Unlike more CRDS directories,  the delivery directory is cross-mounted
to pipeline machines which handle archiving.  As part of the protocol with the CRDS archiving pipeline,  the catalog
file is renamed to indicate processing status.  When the catalog is finally deleted,  CRDS assumes that archiving
is successful.   See crds.server.interactive.models for more info on the delivery naming protocol.  Note that files
in the delivery directory are linked to the same inode as the CRDS file cache copy of the file,  or,  in the case
of the .cat delivery file lists, to the permanent copy in the catalogs directory.  For references,  linking avoids
substantial I/O overheads associated with multi-gigabyte JWST references.  For catalogs,  linked or not,  like named
file lists should have the same contents in catalogs and deliveries.


\subsection{uploads subdirectory}
\label{server_guide:uploads-subdirectory}
The uploads directory is the default Django file upload directory for simple file uploads.


\subsection{ingest subdirectory}
\label{server_guide:ingest-subdirectory}
The ingest directory tree contains per-submitter subdirectories which are written to by the Django-file-upload
muli-file upload application used on file submission pages.  The user's guide gives instructions enabling submitters
to copy files directly into their per-user subdirectories as an upload bypass for telecommuters.  (This is a work
around for the situation in which a VPN user winds up transparently downloading and then explicitly uploading
references submitted via the web site;  instead,  a submitter places the file directly into their own ingest
directory keeping the file onsite,  then proceeds with the submission on the web server normally.)


\subsection{ingest\_ssb subdirectory}
\label{server_guide:ingest-ssb-subdirectory}
The ingest\_ssb directory tree is the historical generation and/or drop-off point for the files generated by the
jwst\_gentools.   Ingested files are then submitted to the web site.   The server does not directly access this
directory,  it shares space with it.


\section{Server File Private Cache}
\label{server_guide:server-file-private-cache}
The Isilon CRDS cache storage (i.e. CRDS\_PATH for servers) is located similarly to dynamic file storage:
\begin{quote}

e.g. /ifs/crds/jwst/test/file\_cache/\{config,mappings,references\}/\{hst,jwst\}
\end{quote}

Each CRDS server (test or ops) has a full copy (\textasciitilde{}2T allocation) of all operational and historical (CRDS-only)
reference files and rules. The dev servers have a smaller allocation which is generally linked to /grp/crds
(synced from ops servers) rather than internally stored.

The server file cache config area is generally updated transparently by running cronjobs.   The server file\_cache
and delivery areas are updated as a result of file submissions and archive activity.  Once global Isilon archive storage
becomes available, cache space can be reclaimed by symlinking the CRDS cache path to the global storage rather than
maintaining an internal copy;  there should be a lag of a couple weeks to a month between submission and reclamation
during which the potentially transient file is fully stored in the CRDS server.   Because the CRDS server caches also
contain unconfirmed and unarchived files,  they are currently read protected from anyone except crds.crdsoper.

See the User's manual in the ? on the web sites for more info on the CRDS cache.


\chapter{Cron Jobs}
\label{server_guide:cron-jobs}
Use shell command:

\begin{Verbatim}[commandchars=\\\{\}]
\% crontab -l
\end{Verbatim}

to dump the current crontab and observe the jobs.   Cronjobs currently produce .log files in the CRDS\_server directory.

To change the cronjobs modify \$\{CRDS\}/CRDS\_server/host/crontab and then do:

\begin{Verbatim}[commandchars=\\\{\}]
\% crontab \$\PYGZob{}CRDS\PYGZcb{}/CRDS\_server/host/crontab
\end{Verbatim}

Note that systems on the same subversion branch on which a crontab is modified and committed will automatically pick
up and use the new crontab during the nightly cron job.

See ``man cron'' or Google for more info on maintaining the cron table and crontab syntax.


\section{nightly.cron.job}
\label{server_guide:nightly-cron-job}
CRDS\_server/hosts/nifghtl directory and executes every night at 3:05 am.  The dev and test versions
of the nightly cron fully rebuild and reinstall the CRDS servers,  with the exceptions of database secret setup,
cron jobs, and .setenv rc\_script scripts.   The nightly cronjob on all servers captures diagnostic information about
the server,  including server configuration, disk quotas and usage, subversion status for detecting uncommitted
changes and observing branch and revision, and cache consistency and orphan file checking.   All of the servers
currently update subversion although the OPS (and often TEST) servers are typically on a static branch.   The dev
and test servers also restart.  Output from the nightly cron is sent to the MAILTO variable defined in the
CRDS\_server/host/crontab file,  currently \href{mailto:jmiller@stsci.edu}{jmiller@stsci.edu}.


\section{monitor\_reprocessing}
\label{server_guide:monitor-reprocessing}
Every 5 minutes CRDS\_server/host/monitor\_reprocessing looks for changes in the CRDS operational context and
does an ``affected datasets'' context-to-context bestrefs comparison when the context changes.   This generates
an e-mail to the \$CRDS\_AFFECTED\_DATASETS\_RECIPIENTS addresses set up by the .setenv file.   bestrefs can require
from 20 seconds to 4-8 hours depending on the number of datasets potentially affected as determined by file
differences.


\section{clear\_expired\_locks}
\label{server_guide:clear-expired-locks}
Somewhat dubious,  this falls into the category of periodic server maintenance,  removing expired instrument locking
records from the server locking database.   Every 5 minutes.  Datatbase locks are considered expired when the current
time exceeds the start time of the lock plus the lock's duration;  since this is an asynchronous event,  the expired
lock records sits around in the database until scrubbed out.   In theory the expired locks are replaceable anyway
but this  routine makes sure they're not sitting around in the database causing confusion.  This does not produce e-mail.


\section{sync\_ops\_to\_grp}
\label{server_guide:sync-ops-to-grp}
Every 10 minutes \emph{sync\_ops\_to\_grp} runs crds.sync to publish the crds ops server to the \textbf{/grp/crds/cache} global readonly
Central Store file cache CRDS currently uses as default for OPUS 2014.3.   This does not produce e-mail.


\chapter{Maintenance Commands}
\label{server_guide:maintenance-commands}
Maintenance commands are typically run from the root of the CRDS\_server checkout.   Changing to the CRDS\_server source
directory can be done like this:

\begin{Verbatim}[commandchars=\\\{\}]
\% server \# cd to the CRDS\_server source code checkout
\% pwd
/crds/data1/dljwstcrdsv1/CRDS\_server
\end{Verbatim}

From here on,  we'll assume commands are executed from this directory.

The default Python environment does not include the CRDS server packages directory.   Additional environment variables
required to run the server and some scripts are sourced like this:

\begin{Verbatim}[commandchars=\\\{\}]
\% source env.csh
\end{Verbatim}


\section{Installing the Server Application}
\label{server_guide:installing-the-server-application}
Running the \emph{./install} script will perform many actions including regenerating the environment definition script
\emph{env.csh}.  Primarily \emph{./install} installs the \emph{crds} (core + client) and \emph{crds.server} packages into
a server specific python directory which is added to PYTHONPATH automatically in \emph{env.csh}.  In addition ./install
instantiates some Apache configuration file templates and copies them to the appropriate installation directories.

The install script is typically run like this:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./install [hst\textbar{}jwst]  [django\textbar{}dev\textbar{}test\textbar{}prod]  \textbar{}\& tee install.\textless{}observatory\textgreater{}.\textless{}use\textgreater{}.err
\end{Verbatim}

For example:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./install hst dev  \textbar{}\& tee install.hst.dev.err
\end{Verbatim}

Running ./install explicitly is required to generate \emph{env.csh} for the first time.  Afterward,  env.csh essentially
knows this server is for ``hst dev''.


\section{CRDS Catalog Initialization}
\label{server_guide:crds-catalog-initialization}
Historically the CRDS server catalogs were initialized many times from existing CDBS and JWST references and
the initial CRDS rules set.  \emph{./init} is rarely used anymore but may still be useful for setting up a Django local
test environment, dubbed the ``django'' usecase.

For the most part the ./init script is tasked with installing the server's initial copy of CRDS rules and initializing
the CRDS file catalog (the crds.server.interactive.models Django database with 19000 CDBS references and CRDS rules):

\begin{Verbatim}[commandchars=\\\{\}]
\% server   \# alias to cd to server source directory
\% ./init [hst\textbar{}jwst] [django\textbar{}dev\textbar{}test\textbar{}ops]
\textless{}enter password for test user\textgreater{}
\end{Verbatim}

\textbf{NOTE:}  At this stage \emph{./init} should not be run on the OPS servers.   For VM-based servers it
has effectively been superceded by \emph{tools/restore\_server} and \emph{tools/mirror\_server}.


\section{Starting and Stopping the Server}
\label{server_guide:starting-and-stopping-the-server}
The CRDS server can be started and stopped like this:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./run
\% ./stop
\end{Verbatim}

The \emph{./run} script starts Apache (many httpd processes) and memcached after which the CRDS server should definitely
be abvailable on it's private port (typically 8001).   The web proxy is provided by an independent system which
is rarely-if-ever unavailable,  but which has historically had a random lag of about 1 minute to (by appearances)
connect with the just started Apache.


\section{Updating and Restarting}
\label{server_guide:updating-and-restarting}
Performing a server update generally revolves around stopping the server,  changing and reinstalling the Django
application, and restarting the server.   This is encapsulated in the \emph{./rerun} script:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./rerun
\end{Verbatim}

This works by sequentially invoking other more basic scripts: ./stop, ./install, ./run.

\emph{./rerun} produces a log file of the voluminous install output as install.\textless{}observatory\textgreater{}.\textless{}usecase\textgreater{}.err.  If things
aren't working coherently,  check the install...err file to verify that no setup functions failed,  as might
happen for a Python syntax error or database schema change.

\textbf{NOTE:}  rerunning the server is an integral part of taking the sever offline and switching to the hidden backup port.
Consequently,  activities such as running tests and mirroring should also be viewed as reinstalling the Django
application.   The reinstall is innocuous because any differences in application source code should be very tightly
controlled, related to switching ports only.   However,  it's still a significant hidden side effect to be aware of
because it has obvious implications when performed on an dirty code base.


\section{Running Server Tests}
\label{server_guide:running-server-tests}
The CRDS server unit tests (\textbf{NOT ADVISABLE FOR OPS}) can be run like this:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./runtests
\end{Verbatim}

additional parameters can be passed to runtests,  for example to select specific tests:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./runtests interactive.tests.Hst.test\_index
\end{Verbatim}

Runtests should not be executed on operational or in-test servers because it has side effects which interfere with
server operation.   Runtests has been modified to switch to a backup port during execution,  but the version of
code necessary will only be deployed with OPUS 2014.3 so it is not yet in operations.

\textbf{NOTE:}    without special arrangements,  server self-tests should not be run on the operational servers.
Self-tests are normally run on the dev and test servers during the nightly cron job at 3 am.

It should be noted that the server unit tests typically do run on the dev and test servers in the nightly
cronjob, generally making them available without waiting on the following day.

The server self-tests exercise most but not all of the Django interactive view code,  JSONRPC code, and basic database
interface to DADSOPS.   Although the interactive (web view) self tests run in a Django test database,  the JSONRPC
tests simply ivoke the CRDS client routines to call to the server and verify results.   Hence,  the JSONRPC code
is effectively tested against a live server,  exercising it just like a normal user.  In addition,  the Django
caching interface is not mocked during testing,  so memcached effects impact the live server.   Consequently,  for
running tests on dev, test, or ops servers,  runtests moves the server to the ``backup port'' where it normally hides
during server restoration or mirroring.   Self-tests are typically run like this:


\section{Django Management Commands}
\label{server_guide:django-management-commands}
Django has a manage.py module which is frequently referenced for server maintenance activities.   In CRDS this is
wrapped as:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./manage \textless{}additional parameters to manage.py\textgreater{}
\end{Verbatim}


\section{Command Line Server Debug}
\label{server_guide:command-line-server-debug}
An Ipython shell which runs in a context similar to the CRDS server can be started like this:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./manage shell
In [1]:
\end{Verbatim}

This shell can be useful for debugging and/or maintaining Django models, view code, JSONRPC routines, or
the database interface to the DADSOPS dataset catalog database (HST).

This shell executes in the same directory/context as the CRDS server,  so it provides the same interactive
environment in which server Django code normally executes.   Consequently server modules and packages tend to
import and function normally for interactive debug;  this happens in a shell processs,  not an Apache process,
so the principle coupling to a running server would be the database and file system... and potentially memcached.


\section{CRDS Catalog Database SQL Commands}
\label{server_guide:crds-catalog-database-sql-commands}
The CRDS reference and rules catalog is implemented as a Django model in crds.server.interactive.models.  Typically
it is accessed by using the models module, classes, and functions.  Nevertheless,  the Django models can be accessed
directly with SQL like this:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./manage dbshell  \# to open a SQL prompt to the CRDS server database
...
mysql\textgreater{} ... SQL commands ...
\end{Verbatim}

The server unit tests are ponderous.  Eventually you may \emph{\textless{}control-c\textgreater{}} and leave behind a junk test
database which blocks subsequent testing.  That can generally cleaned up,  with \textbf{caution},  as follows
for e.g. hst dev:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./manage dbshell
mysql\textgreater{} drop database test\_crds\_hst\_dev;
\end{Verbatim}

\textbf{NOTE}:  the CRDS Catalog is in a Django database which is distinct from the DADSOPS dataset catalog that
CRDS uses to find matching parameters and dataset ids.


\section{Nightly Backup}
\label{server_guide:nightly-backup}
All 6 servers run a nightly backup job at 3 am EST.   The backup dumps the Django database and attempts to capture
transient or unique information in the file system.   The backups make a full copy of all CRDS rules.   The backups
do not contain any references,  and in particular,  no transient references in the process of submission or
confirmation.   Nevertheless,  the backups are extremely useful and appear to be capable of restoring
``yesterday's quiescent server''.

Making a backup is done as follows:

\begin{Verbatim}[commandchars=\\\{\}]
\% tools/backup\_server
\end{Verbatim}

backup\_server results in the generation of backup files which are placed in \textbf{\$\{CRDS\}/server/db\_backups} in a dated
subdirectory with dated names,  and also globally in \$\{HOME\}/backups with generic names.   Both locations should be
considered secret and hidden using file permissions.   Dated backups are persistent,  the backups in \$\{HOME\}/backups
are overwritten every time backup\_server is run.   There are unique files for each server.   The files in
\$\{CRDS\}/server/db\_backups are only visible on that VM.


\section{Restoring Nightly Backups}
\label{server_guide:restoring-nightly-backups}
A relatively recent addition is the tools/restore\_server script.   It is quite simple to restore the nightly backup
of a server:

\begin{Verbatim}[commandchars=\\\{\}]
\% tools/restore\_server
\end{Verbatim}

Conceptually,  restore server reloads the server database and restores the delivery directories and catalogs,  and
removes any reference or rules files orphaned by the database restoration,  those added to the cache since the backup
was made.

As a matter of implementation,  server restoration is handled by mirroring a server to itself.

During the process of restoration,  the server is moved to a hidden backup port and will be seen as temporarily
unavailble through the proxy.

restore\_server utilizes the backup files in \$\{HOME\}/backups,  nominally the ones from the last time backup\_server was
executed.   There is currently no automatic process for appropriately copying the dated backup files from
\$\{CRDS\}/server/db\_backups to \$\{HOME\}/backups so they can be used in server mirroring or restoration.

\textbf{IMPORTANT:}  restore\_server should only be used on the OPS server under duress.   Prior to restoring the OPS server,
review the restore\_server / mirror\_server and attempt to mirror the OPS server down to a DEV server,  then test the
mirrored DEV server both interactively and with runtests.


\section{Server Mirroring}
\label{server_guide:server-mirroring}
The term \emph{server mirroring} is given to the process of transferring the server database and file system state from one
VM and server to another,  effectively making the destination server a copy of the source server.

Typical mirroring flows would be to copy the HST OPS server down to the TEST or DEV server,  or TEST down to DEV.

Server mirroring leverages (nightly or dynamic) server backups by restoring them to different servers.  Afterward,
the sync tool is run to synchronize the destination cache with the source server.   Subsequently,  the tools/orphan\_files
script is run to verify destination server file system consistency with the destination server file catalog.

mirror\_server does not safeguard against it,  but it is almost certainly an error to run mirror\_server on an
OPS VM,  which in all likelihood replaces OPS state with something inferior.   There is one exception:  \emph{restore\_server}
will mirror the OPS server to itself by running \emph{mirror\_server} internally in order to revert OPS to its state at the
time of the nightly backup.

For example,  to copy the test server (hst-crds-test, tlhstcrdsv1) down to the dev server (hst-crds-dev, dlhstcrdsv1),
perform these steps.

First, optionally, on the source server:

\begin{Verbatim}[commandchars=\\\{\}]
\# login tlhstcrdsv1
\% server
\% tools/backup\_server
\end{Verbatim}

That puts required backup files in global (cross-server) \$\{HOME\}/backups.  If this steps is omitted,  the files in
\$\{HOME\}/backups should correspond to the server state at the time of the last backup,  nominally 3 am.  If you're trying
to mirror a change on the test server that you just made,  then immediately backing up the test server is required so
that the change is recorded in the current backup.

Second, on the destination server:

\begin{Verbatim}[commandchars=\\\{\}]
\# login dlhstcrdsv1
\% server
\% tools/mirror\_server hst test https://hst-crds-test.stsci.edu \textbar{}\& tee mirror\_server.hst.test.err
\end{Verbatim}

where the parameters to mirror\_server specify the \emph{source} server and the destination is implicitly the
server of the current login.

Server mirroring requires the source server to be online and available.   The destination server is moved
to a backup port so that it is unavailable while it transitions through various inconsistent states.


\chapter{Delivery Troubleshooting}
\label{server_guide:delivery-troubleshooting}
This section discusses possible operational failure modes and how to handle them.   There are some comaratively simple
problems which may be addressable on an emergency basis.   As a general rule,  for seemingly complex or uncertain
procedures,  first mirror the OPS server to the DEV server,  then perfom the procedure on the DEV server,  then
apply the proven procedure to the OPS server.   For improved certainty,  switch the DEV server to the OPS server
source code branches (CRDS and CRDS\_server),  rerun,  and then perform the procedure on the DEV server.


\section{Remedy by Backup}
\label{server_guide:remedy-by-backup}
For some failure modes it may be desirable to restore the server to the nightly backup for the previous day.  See
\emph{restore\_server} above.

\textbf{NOTE:}  requires server database and file store changes,  restart.

This approach might be particularly effective for temporarily bypassing failed deliveries
by one instrument so that others can proceed,  and also for cleaning up new or failed CRDS rules which are known to
be non-viable.   If failed CRDS rules have already been transferred to the archive,  either removing them from the
archive must be coordinated with DSB and the CRDS Archiving Pipeline,  or restore\_server should not be performed and
the files should be Marked Bad in CRDS instead.   See the CRDS user's guide (on the server) for information about
marking files as bad.


\section{Rmap or Context Fix Required}
\label{server_guide:rmap-or-context-fix-required}
Potentially a best references assignment error could be detected which requires a rules fix.

\textbf{NOTE:} should be possible without OPS server changes.

The procedure for fixing rules should basically be:
\begin{enumerate}
\setcounter{enumi}{-1}
\item {} 
Mirror OPS to DEV server and work using the DEV server.

\item {} 
Sync or download a copy of the rmap file requiring changes.

\item {} 
Correct the rules and test locally using elevated verbosity.  --verbose or --verbosity=100 or something in between.

\item {} 
Upload the modified rmap using Submit Mappings and check ``Generate Context'' to create new instrument and pipeline
mappings which include the new context.

\item {} 
DEV servers do not archive and rules are immediately sync'able and useable.   Sync to a local cache and test.

\item {} 
When satisfied that the DEV server is working,  repeat for the OPS server.  Very possibly the original fixed
copy of the .rmap is directly submissible to OPS.

\item {} 
When the OPS systems have successfully archived the new rules,  test them by syncing and running bestrefs.
The default readonly cache at /grp/crds/cache should sync within 15 minutes of archiving.

\item {} 
Inform \href{mailto:crds\_team@stsci.edu}{crds\_team@stsci.edu} that you think the new rules are working and should either receive a second
opinion or be made operational by the pipeline,  basically Richard Spencer performing Set Context on the
web site.

\end{enumerate}

Context fixes (imap's and pmap's) need to be performed manually,  typically without automatic renaming.   New Context
files are still submitted using Submit Mappings,  but without file renaming or context generation.


\section{Improper Reference File Constraint}
\label{server_guide:improper-reference-file-constraint}
Valid reference files may be rejected due to overly stringent or incorrect matching parameter constraints.

\textbf{NOTE:} requires OPS server file updates, reinstall, and restsart.

The synposis of the fix is to modify the appropriate .tpn and/or \_ld.tpn file in crds.hst.tpns and update and
restart the server.

It's possible that the reference file constraints defined in the CRDS observatory packages will be overly stringent
causing the submission of a valid file to fail.   For HST,  reference constraints are defined in the crds/hst/tpns
directory and define two phases of reference file symbolism.   The first phase,  defined by .tpn files for each
instrument-specific type,  defines reference parameters as they appear in the reference file.  The second phase,
defined by \_ld.tpn files,  define reference parameters as expanded in rmaps by rules in crds.hst.substitutions.
In this scenario,  errors or missing values in the .tpn's need to be fixed.


\section{Improper Reference Parameter Expansion}
\label{server_guide:improper-reference-parameter-expansion}
Valid reference files may be inserted into their corresponding .rmap incorrectly,  most probably identified
by certify warnings about new match tuples in the updated .rmap.

\textbf{NOTE:} requires OPS server file updates, reinstall, and restsart.

The synposis for the fix is to modify substutions.dat in crds.hst,  reinstall the server, and restart.

For HST, reference file matching parameters define where the reference is inserted into .rmaps.
During the reference insertion process,  reference file parameters are expanded using context-sensitive expansion
rules defined in crds/hst/substitutions.dat.  Deficiencies in those rules will result in references being added
to the wrong rmap matching paths.   The short term fix would be to modify substitions.dat,  manually test the rmap
update proces, the resulting rmap, and finally adjusted best references.


\section{Table Row Change Warnings}
\label{server_guide:table-row-change-warnings}
Submission of a new table reference file may result in certify warnings due to comparison with the old version
of the table and deletion of rows.

\textbf{NOTE:}  make it clear warnings are approximate, tripwires, then verify file differences and confirm or cancel.

It should be noted that the warnings are approximate and advisory,  not  definitive.   With that in mind,  verify with
the submitters and/or reference developers that the noted differences are not a problem,  then proceed with
confirmation or rejection.  Row modifications may be perceived by certify as deletions and additions rather than as
replacements.



\renewcommand{\indexname}{Index}
\printindex
\end{document}

% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}

\usepackage{enumitem}
\setlistdepth{15}

\usepackage{amsmath}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}

% In the parameters section, place a newline after the Parameters
% header
\usepackage{expdlist}
\let\latexdescription=\description
\def\description{\latexdescription{}{} \breaklabel}

% Make Examples/etc section headers smaller and more compact
\makeatletter
\titleformat{\paragraph}{\normalsize\py@HeaderFamily}%
            {\py@TitleColor}{0em}{\py@TitleColor}{\py@NormalColor}
\titlespacing*{\paragraph}{0pt}{1ex}{0pt}

% Fix footer/header
\@ifundefined{chaptermark}{%
\newcommand{\chaptermark}[1]{\markboth{\MakeUppercase{\thechapter.\ #1}}{}}
}{%
\renewcommand{\chaptermark}[1]{\markboth{\MakeUppercase{\thechapter.\ #1}}{}}
}

\@ifundefined{chaptermark}{%
\newcommand{\sectionmark}[1]{\markright{\MakeUppercase{\thesection.\ #1}}}
}{%
\renewcommand{\sectionmark}[1]{\markright{\MakeUppercase{\thesection.\ #1}}}
}

\@ifundefined{TSR}{}{%
\renewcommand{\py@HeaderFamily}{\rmfamily\bfseries}
\definecolor{TitleColor}{rgb}{0,0,0}
\renewcommand{\appendix}{\par
  \setcounter{section}{0}%
  \inapptrue%
  \renewcommand\thesection{\@Alph\c@section}}
}

\makeatother

% Make the pages always arabic, and don't do any pages without page
% numbers
\pagenumbering{arabic}
\pagestyle{plain}


\title{CRDS Server Documentation}
\date{July 16, 2014}
\release{1.1}
\author{STScI}
\newcommand{\sphinxlogo}{\includegraphics{stsci_logo.pdf}\par}
\renewcommand{\releasename}{Release}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\def\PYG@tok@gd{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\def\PYG@tok@gu{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\def\PYG@tok@gt{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.25,0.82}{##1}}}
\def\PYG@tok@gs{\let\PYG@bf=\textbf}
\def\PYG@tok@gr{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\def\PYG@tok@cm{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\def\PYG@tok@vg{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@m{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@mh{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@cs{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\colorbox[rgb]{1.00,0.94,0.94}{##1}}}
\def\PYG@tok@ge{\let\PYG@it=\textit}
\def\PYG@tok@vc{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@il{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@go{\def\PYG@tc##1{\textcolor[rgb]{0.19,0.19,0.19}{##1}}}
\def\PYG@tok@cp{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@gi{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\def\PYG@tok@gh{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\def\PYG@tok@ni{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\def\PYG@tok@nl{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\def\PYG@tok@nn{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\def\PYG@tok@no{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\def\PYG@tok@na{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@nb{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@nc{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\def\PYG@tok@nd{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\def\PYG@tok@ne{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@nf{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\def\PYG@tok@si{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\def\PYG@tok@s2{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@vi{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@nt{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\def\PYG@tok@nv{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@s1{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@gp{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\def\PYG@tok@sh{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@ow{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@sx{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\def\PYG@tok@bp{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@c1{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\def\PYG@tok@kc{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@c{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\def\PYG@tok@mf{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@err{\def\PYG@bc##1{\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{##1}}}
\def\PYG@tok@kd{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@ss{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\def\PYG@tok@sr{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\def\PYG@tok@mo{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@mi{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@kn{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@o{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\def\PYG@tok@kr{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@s{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@kp{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@w{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\def\PYG@tok@kt{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\def\PYG@tok@sc{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@sb{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@k{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@se{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@sd{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}



\chapter{Introduction}
\label{server_guide:introduction}\label{server_guide::doc}\label{server_guide:crds-server-guide}
This guide is intended to introduce the CRDS servers for the purposes of maintenance and emergency backup for Todd.

Historical Institute contacts for the CRDS servers include:
\begin{itemize}
\item {} 
Todd Miller:    yours truly,  primary CRDS and (sole) CRDS server application developer.

\item {} 
Patrick Taylor: web proxies, ssl support, and server rc/reboot script coordination

\item {} 
Thomas Walker:  initial VM creation, file systems, and Isilon storage setup.

\end{itemize}


\chapter{Servers}
\label{server_guide:servers}

\section{CRDS pseudo-user and group}
\label{server_guide:crds-pseudo-user-and-group}
The CRDS servers run as the no-login user ``crds''.  The CRDS servers are maintained by getting sudo
access to the crds user and the equivalent of ``su crds'' from a normal user account.   The CRDS
crds\_server script contains the following:

\begin{Verbatim}[commandchars=\\\{\}]
ssh -A -t \$1.stsci.edu /usr/bin/sudo /bin/su - crds
\end{Verbatim}

and is invoked like this:

\begin{Verbatim}[commandchars=\\\{\}]
\% crds\_server plhstcrdsv1
\# crds\_server \textless{}VM-hostname\textgreater{}
\end{Verbatim}

The ssh first logs into your normal user account on the server VM, then su's you to CRDS.

Server maintainers need to get membership in group crdsoper and ``sudo su crds'' access on
the appropriate server VMs.

Members of DSB share the crdsoper group and use it to modify the CRDS server file delivery
directory described below.


\section{Virtual Machines and URLs}
\label{server_guide:virtual-machines-and-urls}
The CRDS servers exist on virtual machines,  run Apache servers and Django via mod\_wsgi,
are backed by memcached as a memory caching optimization for frequent traffic.  Currently
there are 6 VMs and servers:  (hst, jwst) x (dev, test, ops):

\begin{tabulary}{\linewidth}{|L|L|L|L|L|}
\hline
\textbf{
observatory
} & \textbf{
use
} & \textbf{
host/vm
} & \textbf{
direct port
} & \textbf{
url
}\\\hline

hst
 & 
django
 & 
localhost
 & 
8000
 & 
\href{http://localhost:8000}{http://localhost:8000}
\\\hline

hst
 & 
dev
 & 
dlhstcrdsv1
 & 
8001
 & 
\href{https://hst-crds-dev.stsci.edu}{https://hst-crds-dev.stsci.edu}
\\\hline

hst
 & 
test
 & 
tlhstcrdsv1
 & 
8001
 & 
\href{https://hst-crds-test.stsci.edu}{https://hst-crds-test.stsci.edu}
\\\hline

hst
 & 
ops
 & 
plhstcrdsv1
 & 
8001
 & 
\href{https://hst-crds.stsci.edu}{https://hst-crds.stsci.edu}
\\\hline

jwst
 & 
django
 & 
localhost
 & 
8000
 & 
\href{http://localhost:8000}{http://localhost:8000}
\\\hline

jwst
 & 
dev
 & 
dljwstcrdsv1
 & 
8001
 & 
\href{https://jwst-crds-dev.stsci.edu}{https://jwst-crds-dev.stsci.edu}
\\\hline

jwst
 & 
test
 & 
tljwstcrdsv1
 & 
8001
 & 
\href{https://jwst-crds-test.stsci.edu}{https://jwst-crds-test.stsci.edu}
\\\hline

jwst
 & 
ops
 & 
pljwstcrdsv1
 & 
8001
 & 
\href{https://jwst-crds.stsci.edu}{https://jwst-crds.stsci.edu}
\\\hline
\end{tabulary}


For debug purposes the servers can be accessed by bypassing the proxy using VM-based URLs such
as \href{https://plhstcrdsv1.stsci.edu:8001/}{https://plhstcrdsv1.stsci.edu:8001/}.  These direct URLs are visible on site only.  Only the OPS
server URLs will be visible off site.

See CRDS\_server/sources/site\_config.py to verify this information.


\chapter{Server File Systems}
\label{server_guide:server-file-systems}

\section{/home/crds}
\label{server_guide:home-crds}
The VMs and servers share a common /home/crds directory which has potential as a single point failure.  In particular,
critical shell rc scripts (.setenv) are shared by all servers and must be updated with extreme care because
any error instantly affects all 6 servers.  Consequently, .setenv should be kept as simple as reasonably
possible.  /home/crds also houses rc\_script.

/home/crds is useful for communicating information between VMs during setup and maintenance.


\section{Server Static File Storage}
\label{server_guide:server-static-file-storage}
The CRDS server code and support files (Python stack, logs, monitor\_reprocessing dir) are stored on
a private VM-unique volume named after the host,  e.g.  /crds/data1.  This serves as the
./configure --prefix directory for a small number of packages not contained in the crds\_stack subdirectory.
Files within this directory tree are logically executable or in some way secret,  sensitive with respect
to server security.   Most files/subdirs are located in a subdirectory named after the host,
e.g. /crds/data1/plhstcrdsv1.


\subsection{Server Runtime Directories}
\label{server_guide:server-runtime-directories}
A number of subdirectories are used to store files related to running Apache, logging, or backups under
e.g. /crds/data1/dlhstcrdsv1/server.


\subsubsection{conf}
\label{server_guide:conf}
Apache config files are installed here at e.g. /crds/data1/dlhstcrdsv1/server/conf.  Files ssl.conf and httpd.conf.


\subsubsection{logs}
\label{server_guide:logs}
There are a number of Apache logs kept at e.g. /crds/data1/dlhstcrdsv1/server/logs.  These logs record requests to
Apache and stderr output from Django views not visible to end users.


\subsubsection{db\_backups}
\label{server_guide:db-backups}
The output of the CRDS\_server/tools/backup\_server script is kept here in dated subdirectories,
e.g. /crds/data1/dlhstcrdsv1/server/db\_backups/2014-05-30-033327.  These contain a backup of the CRDS server database,
catalog files, deliveries files, all mappings, the server CRDS cach config directory, and an VM rpm listing.
For use with restore\_server,  these files would need to be copied to differently named locations in \$HOME/backups
which only record the results of the last backup.


\subsubsection{wsgi-scripts}
\label{server_guide:wsgi-scripts}
The mod\_wsgi script which bridges from Apache to Django,  crds.wsgi,  is kept here,
e.g. /crds/data1/dlhstcrdsv1/server/wsgi-scripts.  Potentially other django or non-django WSGI scripts
would go here as well.


\subsubsection{run}
\label{server_guide:run}
The running Apache process id is stored here.   The id of memcached should be stored here as well but isn't.


\subsection{Database files}
\label{server_guide:database-files}
Files required to support operations with databases are stored in a top level static file system
subdirectory,  e.g. /crds/data1/database.   These files are secret,  effectively mode 700, and maintained
manually as part of database setup.  They're referred to by site-specific database configurations.


\subsection{CRDS}
\label{server_guide:crds}
The checkout of the CRDS core library source code installed with the CRDS server is located in the static file tree
under the subdirectory CRDS and visited using the alias ``crds''.  e.g.  /crds/data1/plhstcrdsv1/CRDS


\subsection{CRDS\_server}
\label{server_guide:crds-server}
The checkout of the CRDS server source code is located in the static file tree under the subdirectory CRDS\_server
and visited using the alias ``server''.  e.g. /crds/data1/plhstcrdsv1/CRDS\_server


\subsubsection{host}
\label{server_guide:host}
The CRDS\_server/host subdirectory is on the PATH.  It contains scripts related to cron jobs,  affected datasets
reprocessing, stack building,  server utilities, etc.   e.g. /crds/data1/plhstcrdsv1/CRDS\_server/host


\subsubsection{tools}
\label{server_guide:tools}
The CRDS\_server/tools directory contains more complicated scripts related to server backup, restore, mirroring,
consistency checking, server initialization, user and group maintenance, etc.   The tools directory is not on the
PATH and contains more eclectic scripts developed in an unplanned manner,  basically capturing whatever I needed
to do repeatedly or had to Google.   e.g. /crds/data1/plhstcrdsv1/CRDS\_server/tools


\subsubsection{servers}
\label{server_guide:id1}
This directory contains the Apache and mod\_wsgi configuration files.  e.g. /crds/data1/plhstcrdsv1/CRDS\_server/servers


\subsubsection{sources}
\label{server_guide:sources}
This directory contains the Django server and application source code.   e.g. /crds/data1/plhstcrdsv1/CRDS\_server/sources
\begin{itemize}
\item {} \begin{description}
\item[{\emph{sources/configs} contains site specific django configuration and database configuration files.  The appropriate files}] \leavevmode
are copied to sources/site\_config.py and sources/crds\_database.py at install time.   Those are then
imported into more generic configuration files sources/config.py and sources/settings.py.   The site
specific files are intended to contain the minimal information required to differentiate servers.

\end{description}

\item {} 
\emph{sources/urls.py}     defines most of the site URLs for all applications.

\item {} 
\emph{sources/settings.py} fairly standard Django settings.py

\item {} 
\emph{sources/templates}  contains web template base classes

\item {} 
\emph{sources/static}     contains most CRDS static files,  particularly Javascript and CSS.

\item {} 
\emph{sources/interactive}  is the primary web application for CRDS browsing and file submission.

\item {} 
\emph{sources/jsonapi}     is the JSONRPC application which supports web services in the crds.client api.

\item {} \begin{description}
\item[{\emph{sources/jpoll}    application supports the Javascript logging + done polling system used for long running views,}] \leavevmode
particularly file submissions which can exceed proxy timeouts and run too long to leave a human
without info.

\end{description}

\item {} 
\emph{sources/locking}     application for database based locks used by CRDS web logins for exclusive access to an instrument.

\item {} 
\emph{sources/fileupload}  application supports the fancy file submission file upload dialogs for file submissions.

\item {} \begin{description}
\item[{\emph{sources/stats}      application mostly defunct django-level request logging to database,  superceded by Apache}] \leavevmode
logging.  Some parameter capture not present in current Apache configuration.

\end{description}

\end{itemize}


\subsection{crds\_stacks}
\label{server_guide:crds-stacks}
The crds\_stacks subdirectory contains stock python stacks and source code.  The CRDS server Python stack is built
from source contained in the installer3 subdirectory.  An automatic nightly build and reinstall of the stack occurs on the
dev and test servers so it's possible to upgrade all the non-ops servers by updating the installer3 repo.  The
master copy of the CRDS server installer3 repo is contained in /eng/ssb/crds/installer3.   Independent checkouts
of the repo are contained in the stacks file store for each VM.   e.g. /crds/data1/plhstcrdsv1/crds\_stacks


\subsection{monitor\_reprocessing}
\label{server_guide:monitor-reprocessing}
Output from the monitor\_reprocessing cron job is stored in dated subdirectories here.  Also the file old\_context.txt
which records the last known operational context against which changes are measured.  Changed old\_context.txt will
trigger an affected datasets calculation as will changing the operational context on the web site.


\section{Server Dynamic File Storage}
\label{server_guide:server-dynamic-file-storage}
For operating,  the CRDS servers require a certain amount of dynamic storage use for purposes like:
\begin{itemize}
\item {} 
holding pending archive deliveries  (deliveries, catalogs)

\item {} 
uploading files (uploads, ingest, ingest\_ssb)

\end{itemize}

The server dynamic file storage is located on the Isilon file server at:
\begin{quote}

/ifs/crds/\textless{}obsevatory\textgreater{}/\textless{}use\textgreater{}/server\_files,    e.g. /ifs/crds/hst/ops/server\_files.
\end{quote}

Since this area is actively written as a consequence of users accessing the web site,  it is kept distinct from the
code and files required to run the server.


\subsection{Catalog Directory}
\label{server_guide:catalog-directory}
Files submitted to the archive generate .cat file lists which are stored permanently in the catalogs directory.
Any file in CRDS is also stored in the server file cache,  so given the .cat file list the delivery can be recreated
by regenerating file links in the deliveries directory.  The catalogs directory is an internal CRDS server data store
which records file lists from past deliveries.


\subsection{Deliveries Directory}
\label{server_guide:deliveries-directory}
The deliveries directory is cross-mounted between the CRDS server VM and CRDS-archive-pipeline machines,  not
necessarily under the same path name.

Files submitted to the archive are placed in the CRDS delivery directory along with a numbered catalog file which
lists the submitted files one per line.   Unlike more CRDS directories,  the delivery directory is cross-mounted
to pipeline machines which handle archiving.  As part of the protocol with the CRDS archiving pipeline,  the catalog
file is renamed to indicate processing status.  When the catalog is finally deleted,  CRDS assumes that archiving
is successful.   See crds.server.interactive.models for more info on the delivery naming protocol.  Note that files
in the delivery directory are linked to the same inode as the CRDS file cache copy of the file,  or,  in the case
of the .cat delivery file lists, to the permanent copy in the catalogs directory.  For references,  linking avoids
substantial I/O overheads associated with multi-gigabyte JWST references.  For catalogs,  linked or not,  like named
file lists should have the same contents in catalogs and deliveries.


\subsection{Uploads Directory}
\label{server_guide:uploads-directory}
The uploads directory is the default Django file upload directory for simple file uploads.


\subsection{Ingest Directory}
\label{server_guide:ingest-directory}
The ingest directory tree contains per-submitter subdirectories which are written to by the Django-file-upload
muli-file upload application used on file submission pages.  The user's guide gives instructions enabling submitters
to copy files directly into their per-user subdirectories as an upload bypass for telecommuters.  (This is a work
around for the situation in which a VPN user winds up transparently downloading and then explicitly uploading
references submitted via the web site;  instead,  a submitter places the file directly into their own ingest
directory keeping the file onsite,  then proceeds with the submission on the web server normally.)


\subsection{Ingest SSB Directory}
\label{server_guide:ingest-ssb-directory}
The ingest\_ssb directory tree is the historical generation and/or drop-off point for the files generated by the
jwst\_gentools.   Ingested files are then submitted to the web site.   The server does not directly access this
directory,  it shares space with it.


\section{Server File Cache}
\label{server_guide:server-file-cache}
Each CRDS server (test or ops) has a full copy (\textasciitilde{}2T allocation) of all operational and historical (CRDS-only)
reference files.   The dev servers have a smaller allocation which is generally linked to /grp/crds
(synced from ops servers) rather than internally stored.  The Isilon CRDS cache storage (i.e. CRDS\_PATH for servers)
is located similarly to dynamic file storage:
\begin{quote}

e.g. /ifs/crds/jwst/test/file\_cache
\end{quote}

The server file cache config area is generally updated transparently by running cronjobs.   The server file\_cache
and delivery areas are updated as a result of file submissions and archive activity.  Once global Isilon archive storage
becomes available, cache space can be reclaimed by symlinking the CRDS cache path to the global storage rather than
maintaining an internal copy;  there should be a lag of a couple weeks to a month between submission and reclamation
during which the potentially transient file is fully stored in the CRDS server.   Because the CRDS server caches also
contain unconfirmed and unarchived files,  they are currently read protected from anyone except crds.crdsoper.

See the User's manual in the ? on the web sites for more info on the CRDS cache.


\chapter{RC scripts}
\label{server_guide:rc-scripts}
The RC scripts are kept with the server source code in the directory ``hosts'' under the names dot\_setenv and
rc\_script.


\section{.setenv}
\label{server_guide:setenv}
The CRDS user runs under /bin/tcsh and executes .setenv for CRDS-server specific initializations.   Note that
\$HOME/.setenv is shared across all CRDS servers and should be modified with extreme caution.


\section{\$HOME/rc\_script}
\label{server_guide:home-rc-script}
The /home/crds/rc\_script is executed to restart the servers,  or shut them down,  whenever the server is rebooted.


\chapter{Cron Jobs}
\label{server_guide:cron-jobs}
Use shell command:

\begin{Verbatim}[commandchars=\\\{\}]
\% crontab -l
\end{Verbatim}

to dump the current crontab and observe the jobs.   Cronjobs currently produce .log files in the CRDS\_server directory.

To change the cronjobs modify \$\{CRDS\}/CRDS\_server/host/crontab and then do:

\begin{Verbatim}[commandchars=\\\{\}]
\% crontab host/crontab
\end{Verbatim}

Note that systems on the same subversion branch on which a crontab is modified and committed will automatically pick
up and use the new crontab during the nightly cron job.

See ``man cron'' or Google for more info on maintaining the cron table and crontab syntax.


\section{nightly.cron.job}
\label{server_guide:nightly-cron-job}
CRDS\_server/hosts/nifghtl directory and executes every night at 3:05 am.  The dev and test versions
of the nightly cron fully rebuild and reinstall the CRDS servers,  with the exceptions of database secret setup,
cron jobs, and .setenv rc\_script scripts.   The nightly cronjob on all servers captures diagnostic information about
the server,  including server configuration, disk quotas and usage, subversion status for detecting uncommitted
changes and observing branch and revision, and cache consistency and orphan file checking.   All of the servers
currently update subversion although the OPS (and often TEST) servers are typically on a static branch.   The dev
and test servers also restart.  Output from the nightly cron is sent to the MAILTO variable defined in the
CRDS\_server/host/crontab file,  currently \href{mailto:jmiller@stsci.edu}{jmiller@stsci.edu}.


\section{monitor\_reprocessing}
\label{server_guide:id2}
Every 5 minutes CRDS\_server/host/monitor\_reprocessing looks for changes in the CRDS operational context and
does an ``affected datasets'' context-to-context bestrefs comparison when the context changes.   This generates
an e-mail to the \$CRDS\_AFFECTED\_DATASETS\_RECIPIENTS addresses set up by the .setenv file.   bestrefs can require
from 20 seconds to 4-8 hours depending on the number of datasets potentially affected as determined by file
differences.


\section{clear\_expired\_locks}
\label{server_guide:clear-expired-locks}
Somewhat dubious,  this falls into the category of periodic server maintenance,  removing expired instrument locking
records from the server locking database.   Every 5 minutes.  Datatbase locks are considered expired when the current
time exceeds the start time of the lock plus the lock's duration;  since this is an asynchronous event,  the expired
lock records sits around in the database until scrubbed out.   In theory the expired locks are replaceable anyway
but this  routine makes sure they're not sitting around in the database causing confusion.  This does not produce e-mail.


\section{sync\_ops\_to\_grp}
\label{server_guide:sync-ops-to-grp}
Every 10 minutes \emph{sync\_ops\_to\_grp} runs crds.sync to publish the crds ops server to the \textbf{/grp/crds/cache} global readonly
Central Store file cache CRDS currently uses as default for OPUS 2014.3.   This does not produce e-mail.


\chapter{Maintenance Commands}
\label{server_guide:maintenance-commands}
Maintenance commands are typically run from the \textbf{\$CRDS/CRDS\_server directory},  the root of the CRDS\_server checkout.

The default Python environment does not include the CRDS server packages directory.   Many of the shell scripts
described below are either generated by the \$\{CRDS\}/crds\_server/install script or are dependent on scripts and
config files that are generated.   Hence, this section will only make sense in a CRDS server environment which has
been initialized at least once.


\section{Installing the Server Application}
\label{server_guide:installing-the-server-application}
Running the ./install script will perform many actions including regenerating the environment definition script
\emph{\$\{CRDS\}/CRDS\_server/env.csh}.  Primarily ./install installs the \emph{crds} client and \emph{crds.server} packages into
a server specific python directory which is added to PYTHONPATH automatically in \emph{env.csh}.  In addition ./install
instantiates some Apache configuration file templates and copies them to the appropriate installation directories.

The install script is typically run like this:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./install [hst\textbar{}jwst]  [django\textbar{}dev\textbar{}test\textbar{}prod]  \textbar{}\& tee install.\textless{}observatory\textgreater{}.\textless{}use\textgreater{}.err
\end{Verbatim}

For example:

\begin{Verbatim}[commandchars=\\\{\}]
\% server   \# alias to cd to server source directory
\% ./install hst dev  \textbar{}\& tee install.hst.dev.err
\end{Verbatim}

Running ./install explicitly is required to generate \emph{env.csh} for the first time.  Afterward,  env.csh essentially
knows this server is for ``hst dev''.


\section{Starting and Stopping the Server}
\label{server_guide:starting-and-stopping-the-server}
The CRDS server can be started and stopped like this:

\begin{Verbatim}[commandchars=\\\{\}]
\% server   \# alias to cd to server source directory
\% ./run
\% ./stop
\end{Verbatim}


\section{Re-installing the CRDS Server}
\label{server_guide:re-installing-the-crds-server}
Once the CRDS server has been initialized, the server can be stopped, updated/re-installed, and retarted with
the \emph{rerun} command:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./rerun
\end{Verbatim}

This process does not affect the server Python stack.


\section{Running Server Tests}
\label{server_guide:running-server-tests}
The CRDS server unit tests (\textbf{NOT ADVISABLE FOR OPS}) can be run like this:

\begin{Verbatim}[commandchars=\\\{\}]
\% server   \# alias to cd to server source directory
\% ./runtests
\end{Verbatim}

additional parameters can be passed to runtests,  for example to select specific tests:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./runtests interactive.tests.Hst.test\_index
\end{Verbatim}

Runtests should not be executed on operational or in-test servers because it has side effects which interfere with
server operation.   Runtests has been modified to switch to a backup port during execution,  but the version of
code necessary will only be deployed with OPUS 2014.3 so it is not yet in operations.

The server unit tests are ponderous.  Eventually you may \emph{\textless{}control-c\textgreater{}} and leave behind a junk test
database which blocks subsequent testing.  That can generally cleaned up,  with \textbf{extreme caution},  as follows
for e.g. hst dev:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./manage dbshell
mysql\textgreater{} drop database test\_crds\_hst\_dev;
\end{Verbatim}

It should be noted that the server unit tests typically do run on the dev and test servers in the nightly
cronjob, generally making them available without waiting on the following day.


\section{CRDS catalog initializaton}
\label{server_guide:crds-catalog-initializaton}
Historically the CRDS server catalogs were initialized many times from existing CDBS and JWST references and
the initial CRDS rules set.  \emph{./init} is rarely used anymore but may still be useful for setting up a Django local
test environment, typically called a ``dev'' server except in CRDS the ``dev'' server is an actual VM so Django dev
servers are referred to as ``django'' servers.

For the most part the ./init script is tasked with installing the server's initial copy of CRDS rules and initializing
the CRDS file catalog (the crds.server.interactive.models Django database with 19000 CDBS references and CRDS rules):

\begin{Verbatim}[commandchars=\\\{\}]
\% ./init [hst\textbar{}jwst] [django\textbar{}dev\textbar{}test\textbar{}ops]
\textless{}enter password for test user\textgreater{}
\end{Verbatim}

\textbf{NOTE:}  At this stage \emph{./init} should not be run on the OPS servers.   For operational servers it
has effectively been superceded by \textbf{tools/restore\_server} and \textbf{tools/mirror\_server}.


\section{Starting the server}
\label{server_guide:starting-the-server}
The \emph{./run} script starts Apache (many httpd processes) and memcached after which the CRDS server should definitely
be abvailable on it's private port (typically 8001).   The web proxy is provided by an independent system which
is rarely-if-ever unavailable:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./run
\end{Verbatim}


\section{Updating and Restarting}
\label{server_guide:updating-and-restarting}
Performing a server update often revolves around stopping the server,  changing and reinstalling the Django
application, and restarting the server:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./rerun
\end{Verbatim}

This works by sequentially invoking other more basic scripts: ./stop, ./install, ./run.

\emph{./rerun} produces a log file of the voluminous install output as install.\textless{}observatory\textgreater{}.\textless{}usecase\textgreater{}.err.  If things
aren't working coherently,  check the install...err file to verify that no setup functions failed,  as might
happen for a Python syntax error.


\section{Running Server Self-tests}
\label{server_guide:running-server-self-tests}
The server self-tests exercise most but not all of the Django interactive view code,  JSONRPC code, and basic database
interface to DADSOPS.   Although the interactive (web view) self tests run in a Django test database,  the JSONRPC
tests simply ivoke the CRDS client routines to call to the server and verify results.   Hence,  the JSONRPC code
is effectively tested against a live server,  exercising it just like a normal user.  In addition,  the Django
caching interface is not mocked during testing,  so memcached effects impact the live server.   Consequently,  for
running tests on dev, test, or ops servers,  runtests moves the server to the ``backup port'' where it normally hides
during server restoration or mirroring.   Self-tests are typically run like this:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./runtests   \# (in second terminal window if using a Django dev local server)
\end{Verbatim}

\textbf{NOTE:}    without special arrangements,  server self-tests should not be run on the operational servers.
Self-tests are normally run on the dev and test servers during the nightly cron job at 3 am.


\section{Django Management Commands}
\label{server_guide:django-management-commands}
Django has a manage.py module which is frequently referenced for server maintenance activities.   In CRDS this is
wrapped as:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./manage \textless{}additional parameters to manage.py\textgreater{}
\end{Verbatim}


\section{Command Line Server Debug}
\label{server_guide:command-line-server-debug}
An Ipython shell can be started in either of these equivalent ways:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./manage shell
\end{Verbatim}

This shell can be useful for debugging and/or maintaining Django models, view code, JSONRPC routines, or
the database interface to the DADSOPS dataset catalog database (HST).

This shell executes in the same directory/context as the CRDS server,  so it provides the same interactive
environment in which server Django code normally executes.   Consequently server modules and packages tend to
import and function normally for interactive debug;  this happens in a shell processs,  not an Apache process,
so the principle coupling to a running server would be the database and file system... and potentially memcached.


\section{CRDS Catalog Database SQL Commands}
\label{server_guide:crds-catalog-database-sql-commands}
The CRDS reference and rules catalog is implemented as a Django model in crds.server.interactive.models.  Typically
it is accessed by using the models module, classes, and functions.  Nevertheless,  the Django models can be accessed
directly with SQL like this:

\begin{Verbatim}[commandchars=\\\{\}]
\% ./manage dbshell  \# to open a SQL prompt to the CRDS server database
...
mysql\textgreater{} ... SQL commands ...
\end{Verbatim}

\textbf{NOTE}:  the CRDS Catalog is in a Django database which is distinct from the DADSOPS dataset catalog that
CRDS uses to find matching parameters and dataset ids.


\section{Nightly Backup}
\label{server_guide:nightly-backup}
All 6 servers run a nightly backup job at 3 am EST.   The backup dumps the Django database and attempts to capture
transient or unique information in the file system.   The backups make a full copy of all CRDS rules.   The backups
do not contain any references,  and in particular,  no transient references in the process of submission or
confirmation.   Nevertheless,  the backups are extremely useful and appear to be capable of restoring
``yesterday's quiescent server''.

Making a backup is done as follows:

\begin{Verbatim}[commandchars=\\\{\}]
\% tools/backup\_server
\end{Verbatim}

backup\_server results in the generation of backup files which are placed in \textbf{\$\{CRDS\}/server/db\_backups} in a dated
subdirectory with dated names,  and also globally in \$\{HOME\}/backups with generic names.   Both locations should be
considered secret and hidden using file permissions.   Dated backups are persistent,  the backups in \$\{HOME\}/backups
are overwritten every time backup\_server is run.   There are unique files for each server.   The files in
\$\{CRDS\}/server/db\_backups are only visible on that VM.


\section{Restoring Nightly Backups}
\label{server_guide:restoring-nightly-backups}
A relatively recent addition is the tools/restore\_server script.   It is quite simple to restore the nightly backup
of a server:

\begin{Verbatim}[commandchars=\\\{\}]
\% tools/restore\_server
\end{Verbatim}

Conceptually,  restore server reloads the server database and restores the delivery directories and catalogs,  and
removes any reference or rules files orphaned by the database restoration,  those added to the cache since the backup
was made.

As a matter of implementation,  server restoration is handled by mirroring a server to itself.

During the process of restoration,  the server is moved to a hidden backup port and will be seen as temporarily
unavailble through the proxy.

restore\_server utilizes the backup files in \$\{HOME\}/backups,  nominally the ones from the last time backup\_server was
executed.   There is currently no automatic process for appropriately copying the dated backup files from
\$\{CRDS\}/server/db\_backups to \$\{HOME\}/backups so they can be used in server mirroring or restoration.

\textbf{IMPORTANT:}  restore\_server should only be used on the OPS server under duress.   Prior to restoring the OPS server,
review the restore\_server / mirror\_server and attempt to mirror the OPS server down to a DEV server,  then test the
mirrored DEV server both interactively and with runtests.


\section{Server Mirroring}
\label{server_guide:server-mirroring}
The term \emph{server mirroring} is given to the process of transferring the server database and file system state from one
VM and server to another,  effectively making the destination server a copy of the source server.

Typical mirroring flows would be to copy the HST OPS server down to the TEST or DEV server,  or TEST down to DEV.

Server mirroring leverages (nightly or dynamic) server backups by restoring them to different servers.  Afterward,
the sync tool is run to synchronize the destination cache with the source server.   Subsequently,  the tools/orphan\_files
script is run to verify destination server file system consistency with the destination server file catalog.

mirror\_server does not safeguard against it,  but it is almost certainly an error to run mirror\_server on an
OPS VM,  which in all likelihood replaces OPS state with something inferior.   There is one exception:  \emph{restore\_server}
will mirror the OPS server to itself by running \emph{mirror\_server} internally in order to revert OPS to its state at the
time of the nightly backup.

For example,  to copy the test server (hst-crds-test, tlhstcrdsv1) down to the dev server (hst-crds-dev, dlhstcrdsv1),
perform these steps.  First, optionally, on the source server:

\begin{Verbatim}[commandchars=\\\{\}]
\# login tlhstcrdsv1
\% server
\% tools/backup\_server
\end{Verbatim}

That puts required backup files in global (cross-server) \$\{HOME\}/backups.  If this steps is omitted,  the files in
\$\{HOME\}/backups should correspond to the server state at the time of the last backup,  nominally 3 am.  If you're trying
to mirror a change on the test server that you just made,  then immediately backing up the test server is required so
that the change is recorded in the current backup.

Second, on the destination server:

\begin{Verbatim}[commandchars=\\\{\}]
\# login dlhstcrdsv1
\% server
\% tools/mirror\_server hst test https://hst-crds-test.stsci.edu \textbar{}\& tee mirror\_server.hst.test.err
\end{Verbatim}

where the parameters to mirror\_server specify the \emph{source} server and the destination is implicitly the
server of the current login.

Server mirroring requires the source server to be online and available.   The destination server is moved
to a backup port so that it is unavailable while it transitions through various inconsistent states.



\renewcommand{\indexname}{Index}
\printindex
\end{document}
